Week 4 Quiz
===================================
Refer to Week 4 Quiz.doc document

================================
Q1 Answer -----------
RF Accuracy = 0.6082  
GBM Accuracy = 0.5152 
Agreement Accuracy = 0.6361
------------------------------
Q1 Work Notes
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test) 

# Set the variable y to be a factor variable in both the training and test set. 
# Then set the seed to 33833. Fit (1) a random forest predictor relating the 
# factor variable y to the remaining variables and (2) a boosted predictor using 
# the "gbm" method. Fit these both with the train() command in the caret package. 

vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)

set.seed(33833)

# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")

# predict test
predict1 <- predict(fit1, newdata = vowel.test)
predict2 <- predict(fit2, newdata = vowel.test)

# combine predictions
DF_combined <- data.frame(predict1, predict2, y = vowel.test$y)
fit_combined <- train(y ~ ., data = DF_combined, method = "gam")
predict3 <- predict(fit_combined, newdata = vowel.test)

# confusion matrixes
c1 <- confusionMatrix(predict1, vowel.test$y)
c2 <- confusionMatrix(predict2, vowel.test$y)
c3 <- confusionMatrix(predict3, DF_combined$y)

# combining didn't give right result?

================================
Q2
Load the Alzheimer's data using the following commands
---code---
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]

Set the seed to 62433 and 
- predict diagnosis with all the other variables 
- using a random forest ("rf"), 
- boosted trees ("gbm") and 
- linear discriminant analysis ("lda") model. 

Stack the 3 predictions together using random forests ("rf"). 
What is the resulting accuracy on the test set? 
Is it better or worse than each of the individual predictions? 

Stacked Accuracy: 0.80 is better than all three other methods
Stacked Accuracy: 0.80 is better than random forests and lda and the same as boosting. <<-- Next guess
Stacked Accuracy: 0.80 is worse than all the other methods. 
Stacked Accuracy: 0.88 is better than all three other methods. <<-- First Guess - Incorrect

--------------------------
set.seed(62433)
mod1 <- train(diagnosis ~.,method="rf",data=training) 
print(mod1)
  mtry  Accuracy   Kappa    
    2   0.7534273  0.2231866
   68   0.8235300  0.5188859 <<- optimal model accuracy .82
  134   0.8182994  0.5104219


set.seed(62433)
mod2 <- train(diagnosis ~ ., method="gbm",data=training,verbose=FALSE)
print(mod2)
  interaction.depth  n.trees  Accuracy   Kappa    
  1                   50      0.8253653  0.5133427
  1                  100      0.8400997  0.5678443
  1                  150      0.8437781  0.5809580
  2                   50      0.8458323  0.5794488
  2                  100      0.8458041  0.5854780
  2                  150      0.8479938  0.5932087
  3                   50      0.8401258  0.5606989
  3                  100      0.8489123  0.5912078
  3                  150      0.8510331  0.5978976 <<- optimal model accuracy .85
pred2<-predict(mod2,testing)

set.seed(62433)
mod3 <- train(diagnosis~.,data=training,method="lda")
print(mod3)
  Accuracy   Kappa    
  0.7287689  0.3846304


pred1<-predict(mod1,testing)
pred1ModFit<-train(diagnosis~.,method="rf",data=testing)

pred2<-predict(mod2,testing)
pred3<-predict(mod3,testing)
predDF<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
combModFit<-train(diagnosis~.,method="rf",data=predDF)
print(combModFit)
  mtry  Accuracy   Kappa    
  2     0.7895211  0.4457389
  3     0.7923642  0.4517710
combPred<- predict(combModFit,predDF)

---------------- 2nd try
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]

# Set the seed to 62433 and predict diagnosis with all the other variables using 
# a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis 
# ("lda") model. Stack the predictions together using random forests ("rf"). 
# What is the resulting accuracy on the test set? Is it better or worse than 
# each of the individual predictions?

set.seed(62433)

# create models
fit1 <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(diagnosis ~ ., data = training, method = "gbm")
fit3 <- train(diagnosis ~ ., data = training, method = "lda")

# predict test
predict1 <- predict(fit1, newdata = testing)
predict2 <- predict(fit2, newdata = testing)
predict3 <- predict(fit3, newdata = testing)

# combine predictions
DF_combined <- data.frame(predict1, predict2, predict3, diagnosis = testing$diagnosis) # training$diagnosis?
fit_combined <- train(diagnosis ~ ., data = DF_combined, method = "rf")
predict4 <- predict(fit_combined, newdata = testing)

# confusion matrixes
c1 <- confusionMatrix(predict1, testing$diagnosis)
c2 <- confusionMatrix(predict2, testing$diagnosis)
c3 <- confusionMatrix(predict3, testing$diagnosis)
c4 <- confusionMatrix(predict4, testing$diagnosis)

print(paste(c1$overall[1], c2$overall[1], c3$overall[1], c4$overall[1]))



==================================
Q3 Answer

FineAggregate
Cement. ê First Guess - Correct
CoarseAggregate
BlastFurnaceSlag
--------------------------------
Question 3 Work Notes
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]

# Set the seed to 233 and fit a lasso model to predict Compressive Strength. 
# Which variable is the last coefficient to be set to zero as the penalty increases? 
# (Hint: it may be useful to look up ?plot.enet).
library(caret)

set.seed(233)


fit <- train(CompressiveStrength ~ ., data = training, method = "lasso")

# Since we are interested in the shrinkage of coefficients as the penalty(lambda) increases, "
# penalty" looks promising for an xvar argument value.
plot.enet(fit$finalModel, xvar = "penalty", use.color = TRUE)

=================================
Q4
Load the data on the number of visitors to the instructors blog from here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv
Using the commands:
----code-----------
library(lubridate) # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)

Fit a model using the bats() function in the forecast package to the training time series. Then forecast this model for the remaining time points. For how many of the testing points is the true value within the 95% prediction interval bounds? 

96%  <<-Next Guess
98%
94%. ê First Guess - Incorrect
93%

----------------------
Question 4 Work Notes

library(lubridate) # For year() function below
install.packages(forecast)
library(forecast)
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)

# fit a model
fit <- bats(tstrain)

# check how long the test set is, so you can predict beyond trainign
h <- dim(testing)[1]

# forecast the model for remaining time points
fcast <- forecast(fit, level = 95, h = h)

# get the accuracy
accuracy(fcast, testing$visitsTumblr)

# check what percentage of times that the actual number of visitors was within
# 95% confidence interval

result <- c()
l <- length(fcast$lower)

for (i in 1:l){
  x <- testing$visitsTumblr[i]
  a <- fcast$lower[i] < x & x < fcast$upper[i]
  result <- c(result, a)
}

sum(result)/l * 100
[1] 96.17021

=============================
Q5
Load the concrete data with the commands:
---code------
set.seed(3523)
library(AppliedPredictiveModeling)
library(caret)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4
    )[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]

Set the seed to 325 and fit a support vector machine using the e1071 package to predict Compressive Strength using the default settings. 
Predict on the testing set. 

What is the RMSE?

11543.39
107.44  ê First Guess - Incorrect
6.72
6.93

--------------------------------
Question 5 Work Notes

set.seed(3523)
library(AppliedPredictiveModeling)
library(caret)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]

# Set the seed to 325 and fit a support vector machine using the e1071 package 
# to predict Compressive Strength using the default settings. Predict on the 
# testing set. What is the RMSE?

set.seed(325)
library(e1071)
library(caret)

fit <- train(CompressiveStrength ~ ., data = training, method = "svmRadial")

prediction <- predict(fit, testing)

accuracy(prediction, testing$CompressiveStrength)






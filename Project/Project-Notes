Project notes

----------------- using randomForest() --- accuracy .9951 ----------
Random Forests
Using the randomForest() library took about 6 minutes
-- code -- 
fitRF <- randomForest(classe ~ ., data = training)
fitRF
Call:
 randomForest(formula = classe ~ ., data = training) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 0.44%
Confusion matrix:
     A    B    C    D    E class.error
A 3902    2    0    1    1 0.001024066
B    9 2643    6    0    0 0.005643341
C    0   12 2384    0    0 0.005008347
D    0    0   24 2227    1 0.011101243
E    0    0    0    4 2521 0.001584158

## test prediction
> predRF <- predict(fitRF, validation)
> confusionMatrix(validation$classe, predRF)
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1673    0    1    0    0
         B    1 1138    0    0    0
         C    0   10 1014    2    0
         D    0    0   13  951    0
         E    0    0    0    2 1080

Overall Statistics
                                          
               Accuracy : 0.9951          
                 95% CI : (0.9929, 0.9967)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9938          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9994   0.9913   0.9864   0.9958   1.0000
Specificity            0.9998   0.9998   0.9975   0.9974   0.9996
Pos Pred Value         0.9994   0.9991   0.9883   0.9865   0.9982
Neg Pred Value         0.9998   0.9979   0.9971   0.9992   1.0000
Prevalence             0.2845   0.1951   0.1747   0.1623   0.1835
Detection Rate         0.2843   0.1934   0.1723   0.1616   0.1835
Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
Balanced Accuracy      0.9996   0.9955   0.9920   0.9966   0.9998


------------ Random Forest Using Caret -------- accuracy .9952 --------------
Using caret to do random forests model took about 8 minutes with parallel processing
-- code --
modFitRF <- train(x,y,data=training,method="rf",trControl=fitControl) # using x/y syntax
modFit ## test accuracy
## -- note - this method ran in about 8 minutes
-- results --
Random Forest 

13737 samples
   52 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 10988, 10989, 10990, 10990, 10991 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa    
   2    0.9902457  0.9876583
  27    0.9906825  0.9882124
  52    0.9828214  0.9782660

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was mtry = 27.

## test prediction
predFitRF <- predict(modFitRF, validation)
confusionMatrix(validation$classe, predFitRF)
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1673    0    1    0    0
         B    3 1135    1    0    0
         C    0    8 1015    3    0
         D    0    0   10  954    0
         E    0    0    0    2 1080

Overall Statistics
                                          
               Accuracy : 0.9952          
                 95% CI : (0.9931, 0.9968)
    No Information Rate : 0.2848          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.994           
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9982   0.9930   0.9883   0.9948   1.0000
Specificity            0.9998   0.9992   0.9977   0.9980   0.9996
Pos Pred Value         0.9994   0.9965   0.9893   0.9896   0.9982
Neg Pred Value         0.9993   0.9983   0.9975   0.9990   1.0000
Prevalence             0.2848   0.1942   0.1745   0.1630   0.1835
Detection Rate         0.2843   0.1929   0.1725   0.1621   0.1835
Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
Balanced Accuracy      0.9990   0.9961   0.9930   0.9964   0.9998

-------------- using Boosting ----- accuracy .960 -----------
## configure trainControl object
--- code ---
fitControl <- trainControl(method="repeatedcv",number=4,repeats=4,allowParallel=TRUE)
fitGBM <- train(x,y, data=training, method="gbm", trControl=fitControl,verbose = FALSE) ## x/y syntax
## --- note - this ran in 3 minutes with 49 warnings
## -- it appears the x/y syntax may have been a problem
## re-run with standard syntax
fitGBM <- train(classe ~ ., data=training, method="gbm", trControl=fitControl,verbose = FALSE)
fitGBM
#--results--
Stochastic Gradient Boosting 

13737 samples
   52 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (4 fold, repeated 4 times) 
Summary of sample sizes: 10302, 10303, 10302, 10304, 10303, 10302, ... 
Resampling results across tuning parameters:

  interaction.depth  n.trees  Accuracy   Kappa    
  1                   50      0.7526383  0.6863668
  1                  100      0.8221226  0.7748103
  1                  150      0.8542622  0.8155523
  2                   50      0.8539528  0.8149392
  2                  100      0.9062570  0.8813651
  2                  150      0.9299520  0.9113466
  3                   50      0.8944640  0.8663958
  3                  100      0.9395429  0.9235017
  3                  150      0.9584698  0.9474568 <<- best accuracy

Tuning parameter 'shrinkage' was held constant at a value of 0.1
Tuning parameter 'n.minobsinnode' was held constant at
 a value of 10
Accuracy was used to select the optimal model using the largest value.
The final values used for the model were n.trees = 150, interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.
#- code --
## test prediction using validation dataset
predGBM= predict(fitGBM,validation)
confusionMatrix(validation$classe,predGBM)
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1637   20   14    2    1
         B   32 1080   26    1    0
         C    0   33  981   11    1
         D    1    2   42  914    5
         E    4   18    9   12 1039

Overall Statistics
                                          
               Accuracy : 0.9602          
                 95% CI : (0.9549, 0.9651)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9497          
 Mcnemar's Test P-Value : 3.279e-10       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9779   0.9367   0.9151   0.9723   0.9933
Specificity            0.9912   0.9875   0.9907   0.9899   0.9911
Pos Pred Value         0.9779   0.9482   0.9561   0.9481   0.9603
Neg Pred Value         0.9912   0.9846   0.9813   0.9947   0.9985
Prevalence             0.2845   0.1959   0.1822   0.1597   0.1777
Detection Rate         0.2782   0.1835   0.1667   0.1553   0.1766
Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
Balanced Accuracy      0.9846   0.9621   0.9529   0.9811   0.9922



